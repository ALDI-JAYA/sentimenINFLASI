{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "092ec33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, url_for,request\n",
    "import logging\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "from pandas.core.computation.pytables import Term\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import collections, numpy\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a9d531",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ALDITR~1\\AppData\\Local\\Temp/ipykernel_3708/68368851.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrender_template\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'result.html'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediksi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m \u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/home\")\n",
    "def home():\n",
    "    return render_template(\"home.html\")\n",
    "def prediksi():\n",
    "    df = pd.read_csv(\"inflasi_training.csv\",encoding=\"ISO-8859-1\",on_bad_lines='skip')\n",
    "    \n",
    "    s_positif = df[df['label'] == 'positif'].sample(247,replace=True)\n",
    "    s_netral = df[df['label'] == 'netral'].sample(400,replace=True)\n",
    "    s_negatif = df[df['label'] == 'negatif'].sample(245,replace=True)\n",
    "    df = pd.concat([s_positif,s_negatif,s_netral])\n",
    "    print(df.shape)\n",
    "    print(df['label'].value_counts(normalize=True))\n",
    "    \n",
    "    def remove_pattern(tweet, pattern):\n",
    "        r = re.findall(pattern, tweet)\n",
    "        for i in r:\n",
    "            tweet = re.sub(i,'', tweet)\n",
    "        return tweet    \n",
    "    df['remove_user'] = np.vectorize(remove_pattern)(df['teks'], \"@[\\w]*\")\n",
    "    def remove(teks):\n",
    "  # Casefolding - mengubah ke case yang sama semua (misal huruf kecil semua)\n",
    "        teks = teks.lower()\n",
    "  #Cleaning\n",
    "        teks = re.sub('[0-9]+', '', teks)#menghilangkan Angka\n",
    "        teks = re.sub(r'\\$\\w*', '', teks)\n",
    "        teks = re.sub(r'rt :[\\s]+', '', teks)\n",
    "        teks = re.sub(\"b'|b\\\"\",'',teks)\n",
    "        teks = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\",\"\", teks)\n",
    "        teks = teks.translate(str.maketrans('','',string.punctuation))#Menghapus karakter tanda baca.\n",
    "        teks = re.sub('<.*?>','',teks)\n",
    "        teks = re.sub(\"\\n\",\" \",teks)\n",
    "        teks = re.sub(r\"\\b[a-zA-Z]\\b\",\"\",teks)\n",
    "        teks = re.sub(r',','',teks)\n",
    "        teks = re.sub(r'#', '', teks)#menghilangkan Hastag\n",
    "        teks = ' '.join(teks.split())\n",
    "        teks = teks.encode('ascii', 'replace').decode('ascii')\n",
    "        return teks\n",
    "    df['clean_tweet'] = df['remove_user'].apply(lambda x: remove(x))\n",
    "    df.drop_duplicates(subset =\"clean_tweet\", keep = 'first', inplace = True)\n",
    "    #df.drop_duplicates(subset =\"teks\", keep = 'first', inplace = True)\n",
    "    df['clean_tweet'].to_csv('data_bersih_111.csv',encoding='utf8', index=False)\n",
    "\n",
    "    def tokenizing(teks):\n",
    "        nstr = teks.split(' ')\n",
    "        dat = []\n",
    "        a = -1\n",
    "        for hu in nstr:\n",
    "            a = a+1\n",
    "        if hu == '':\n",
    "            dat.append(a)\n",
    "        p = 0\n",
    "        b = 0\n",
    "        for q in dat:\n",
    "            b = q - p\n",
    "            del nstr[b]\n",
    "            p = p + 1\n",
    "        return nstr\n",
    "    df['Tokenizing'] = df['clean_tweet'].apply(lambda x: tokenizing(x.lower()))\n",
    "\n",
    "    def freqDist_wrapper(text):\n",
    "        return FreqDist(text)\n",
    "\n",
    "    df['tweet_tokens_fdist'] = df['Tokenizing'].apply(freqDist_wrapper)\n",
    "\n",
    "    print('Frequency Tokens : \\n') \n",
    "    print(df['tweet_tokens_fdist'].head().apply(lambda x : x.most_common()))\n",
    "\n",
    "    list_stopwords = stopwords.words('indonesian')\n",
    "    print(len(list_stopwords))\n",
    "    list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo','ko','gi','tu','ni','pt','yu','yas', \n",
    "                           'kalo', 'amp', 'biar', 'bikin', 'bilang','ie','to','im','ii','da','pj', \n",
    "                           'gak', 'ga', 'krn', 'nya', 'nih', 'sih',\"loh\",\"xfxfxx\",'iv',\n",
    "                            'si', 'tau', 'tdk', 'tuh', 'utk', 'ya','lh',\"nah\",\"xfxfxcxbf\", \n",
    "                           'jd', 'jgn', 'sdh', 'aja', 'n', 't',\"amp\",\"hmm\",'dah','sok','sbb','ayo',\n",
    "                           'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt','hehehe',\n",
    "                           '&amp', 'yah','guys','hai','iii','yuk','kan','cuss','gengss','ges','wk'])\n",
    "    len(list_stopwords)\n",
    "    txt_stopword = pd.read_csv(\"stopword.txt\", names= [\"stopword\"], header = None)\n",
    "    list_stopwords.extend(txt_stopword[\"stopword\"][0].split(' '))\n",
    "    len(list_stopwords)\n",
    "    list_stopwords = set(list_stopwords)\n",
    "    def stopwords_removal(words):\n",
    "        return [word for word in words if word not in list_stopwords]\n",
    "    df['tweet_tokens_WSW'] = df['Tokenizing'].apply(stopwords_removal)\n",
    "\n",
    "    normalisasi = pd.read_csv(\"Normalisasi.csv\",encoding=\"latin1\")\n",
    "    normalisasi_dict = {}\n",
    "    for index, row in normalisasi.iterrows():\n",
    "        if row[0] not in normalisasi_dict:\n",
    "            normalisasi_dict[row[0]]=row[1]\n",
    "\n",
    "    def normalisasi_term(document):\n",
    "        return[normalisasi_dict[term] if term in normalisasi_dict else term for term in document]\n",
    "\n",
    "    df['Normalisasi'] = df['tweet_tokens_WSW'].apply(normalisasi_term)\n",
    "\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    def stemmed_wrapper(term):\n",
    "        return stemmer.stem(term)\n",
    "\n",
    "    term_dict = {}\n",
    "\n",
    "    for document in df['Normalisasi']:\n",
    "        for term in document:\n",
    "            if term not in term_dict:\n",
    "                term_dict[term] = ' '\n",
    "            \n",
    "    print(len(term_dict))\n",
    "    print(\"------------------------\")\n",
    "\n",
    "    for term in term_dict:\n",
    "        term_dict[term] = stemmed_wrapper(term)\n",
    "        print(term,\":\" ,term_dict[term])\n",
    "    \n",
    "    print(term_dict)\n",
    "    print(\"------------------------\")\n",
    "\n",
    "    def get_stemmed_term(document):\n",
    "        return [term_dict[term] for term in document]\n",
    "\n",
    "    df['tweet'] = df['Normalisasi'].swifter.apply(get_stemmed_term)\n",
    "\n",
    "#TF-IDF\n",
    "    feature = df['tweet']\n",
    "    target = df['label']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(feature, target, test_size=0.2, random_state=42)\n",
    "    v_model = TfidfVectorizer().fit(x_train)\n",
    "    v_data = v_model.transform(x_train)\n",
    "\n",
    "\n",
    "    #prediksi\n",
    "    cls_model_mnb = MultinomialNB().fit(v_data,y_train)\n",
    "    cls_model_mnb.score(v_data,y_train)*100\n",
    "    test_data = v_model.transform(x_test)\n",
    "\n",
    "    cls_model_mnb.score(test_data,y_test)*100\n",
    "    prediksi = cls_model_mnb.predict(test_data)\n",
    "    akurasi = cls_model_mnb.score(test_data,y_test)*100\n",
    "\n",
    "    if request.method == 'POST':\n",
    "        message = request.form['message']\n",
    "        data = [message]\n",
    "        model = v_model.transform(data).toarray()\n",
    "        prediksi = cls_model_mnb.predict(model)\n",
    "    return render_template('result.html',prediction = prediksi)\n",
    "\n",
    "app.logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "app.logger.setLevel(logging.ERROR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
